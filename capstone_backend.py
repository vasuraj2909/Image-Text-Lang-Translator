# -*- coding: utf-8 -*-
"""capstone_backend.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LkzBTRm1tuBjW340t98rmZeoyRNtoz76
"""

'''!pip install opencv-python-headless==4.1.2.30
!pip install easyocr

!pip install PyPDF2

!pip install PyMuPDF==1.16.14

!pip install googletrans==3.1.0a0

!pip install beautifulsoup4
!pip install lxml'''

import nltk
nltk.download("stopwords")
nltk.download()

#!pip install flask

"""IMPORT"""

from easyocr import Reader
import PyPDF2
from PyPDF2 import PdfReader
import fitz
from nltk.corpus import stopwords
from nltk.cluster.util import cosine_distance
import numpy as np
#import networkx as nx
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
#from heapq import nlargest
from flask import Flask, render_template

"""INITIALIZATIONS"""

picture = "tamil.png"
pdf = "arabic_text.pdf"

"""IMAGE EXTRACTION"""

# Load model for multiple languages 
reader_en_fr = Reader(['en', 'fr','es'])
reader_ta_en = Reader(['en','ta'])
reader_te_en = Reader(['en','te'])
reader_kn_en = Reader(['en','kn'])
reader_hi_en = Reader(['en','hi'])
reader_ch = Reader(['ch_sim'])
reader_ar = Reader(['ar'])

def read_text(image_name, model_name, in_line=True):

  # Read the data
  text = model_name.readtext(image_name, detail = 0, paragraph=in_line)
  
  # Join texts writing each text in new line
  return '\n'.join(text)

ta_text = read_text(picture, reader_ta_en)
print(ta_text)

ch_text = read_text("chinese.png", reader_ch)
print(ch_text)

ar_text = read_text("arabic.png", reader_ar)
print(ar_text)

"""TO READ PDF"""

# Open the PDF file in read-binary mode
def pdf_read():
    with open(pdf, 'rb') as pdf_file:
        
        # Create a PyPDF2 PdfFileReader object to read the PDF
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        
        # Read all the pages of the PDF and store the text in a variable
        text = ''
        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num]
            text += page.extract_text()
#print(text)

a = input("")
a

"""TRANSLATION"""

def translate_text(text, lang):
        from googletrans import Translator
        translator = Translator()
        translated_text = translator.translate(text, dest='en')
        print(translated_text.text)

"""SUMMARIZE"""

def summarize(txt, n):
    # Tokenize the text into sentences and words
    sentences = sent_tokenize(txt)
    words = word_tokenize(txt)
    
    # Remove stop words from the words list
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word.lower() not in stop_words]
    
    # Calculate the frequency of each word
    freq = nltk.FreqDist(words)
    
    # Calculate the score of each sentence
    scores = {}
    for i, sentence in enumerate(sentences):
        for word in word_tokenize(sentence.lower()):
            if word in freq:
                if i not in scores:
                    scores[i] = freq[word]
                else:
                    scores[i] += freq[word]
                    
    # Get the top n sentences with the highest scores
    top_sentences = nlargest(n, scores, key=scores.get)
    
    # Sort the top sentences in the order they appear in the original text
    summary = ' '.join([sentences[i] for i in sorted(top_sentences)])
    return summary

txt = translated_text.text
summary = summarize(txt, 40) # summarize the text into 11 sentences
print(summary)

"""DESIRED CONVERSION """

from googletrans import Translator

translator = Translator()
translated_text1 = translator.translate(summary, dest='fr')
print(translated_text1.text)

"""SAVE OUTPUT AS TXT OF SUMMARIZATION (innitiated when user clicks download)"""

output = translated_text.text

# Open a new file with write mode ('w')
with open('output.txt', 'w') as f:
    # Write the output to the file
    f.write(output)

# Print a message to confirm that the file was saved
print("Output saved output.txt")

print("You can save the output to the dashboard")

